{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.sparse import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import csv\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199715, 20)\n",
      "(199715,)\n"
     ]
    }
   ],
   "source": [
    "X=np.load('tweets_emb_nltk.npy')\n",
    "y=np.load('tweets_sol_nltk.npy')\n",
    "##-------full data\n",
    "X_full=np.load('tweets_emb_nltk_full.npy')\n",
    "y_full=np.load('tweets_sol_nltk_full.npy')\n",
    "#----test data to give\n",
    "X_test=np.load('tweets_emb_nltk_test.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Step 1\n",
    "X1=preprocessing.scale(X)\n",
    "X1_full=preprocessing.scale(X_full)\n",
    "X1_test=preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79886, 20)\n",
      "(79886,)\n"
     ]
    }
   ],
   "source": [
    "## To get even a smaller train set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.6,random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###FOR SGD THINGS for GloVe embedding\n",
    "##maybe add W_regularizer=l2(0.01)\n",
    "def create_model_glove(neurons=15,neurons2=5,\n",
    "                     init_mode='he_uniform',activation='sigmoid',\n",
    "                     learn_rate=0.1,momentum=0.5,dropout_rate=0.3,\n",
    "                     weight_constraint=0, weight_regularizer=0.0001):\n",
    "    \n",
    "    ## the input layer must have the input_dim numbers of input\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=20, init='he_uniform', activation=activation,W_regularizer=l2(weight_regularizer)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons2, init='he_uniform', activation=activation))\n",
    "    model.add(Dense(1, init='he_uniform', activation=activation))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRID SEARCH Learn rate and momentum\n",
    "model = KerasClassifier(build_fn=create_model_glove, nb_epoch=10, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.3,0.5,1]\n",
    "momentum = [ 0,0.1,0.5]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to Tune the Neuron Activation Function\n",
    "model = KerasClassifier(build_fn=create_model_sgd, nb_epoch=10, batch_size=100, verbose=0)\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### How to Tune the Number of Neurons in the Hidden Layer\n",
    "## create model\n",
    "model = KerasClassifier(build_fn=create_model_glove, nb_epoch=10, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [5,10,15,20]\n",
    "neurons2= [5]\n",
    "param_grid = dict(neurons=neurons,neurons2=neurons2)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(4, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(4, 1)\n",
    "neurons=np.asarray(neurons).reshape(4, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(5, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(5, 1)\n",
    "\n",
    "neurons=np.asarray(neurons).reshape(5, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_glove, nb_epoch=10, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [0]\n",
    "weight_regularizer= [0.00001,0.0001,0.001,0.01,0.1,1]\n",
    "dropout_rate = [0.3]\n",
    "param_grid = dict(dropout_rate=dropout_rate,weight_regularizer=weight_regularizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(6, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(6, 1)\n",
    "\n",
    "neurons=np.asarray(weight_regularizer).reshape(6, 1)\n",
    "plt.scatter(weight_regularizer, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(weight_regularizer,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('L2 Regularizer parameter')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.xscale('log', nonposy='clip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INITIALISATION OF W\n",
    "#create model\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=10, batch_size=100, verbose=0)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with data from word2vec\n",
    "X=np.load('X_w2v.npy')\n",
    "y=np.load('y_w2v.npy')\n",
    "\n",
    "X_full=np.load('X_w2v_full.npy')\n",
    "y_full=np.load('y_w2v_full.npy')\n",
    "#\n",
    "X_test=np.load('X_test_w2v.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_rendu=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]>= 0.5:\n",
    "        y_rendu.append(1)\n",
    "    else: y_rendu.append(0)\n",
    "y=np.array(y_rendu)\n",
    "y_rendu2=[]\n",
    "for i in range(len(y_full)):\n",
    "    if y_full[i]>= 0.5:\n",
    "        y_rendu2.append(1)\n",
    "    else: y_rendu2.append(0)\n",
    "y_full=np.array(y_rendu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##----Step1: PCA\n",
    "##principal component via Singular value decompo\n",
    "pca = PCA(n_components=50,svd_solver='full')\n",
    "pca.fit(X)\n",
    "X1=pca.transform(X)\n",
    "X1_test=pca.transform(X_test)\n",
    "##print the percentage of variance captured with those features\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1_full=pca.transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- Step2 : scaling data\n",
    "\n",
    "X2=preprocessing.scale(X1)\n",
    "X2_test=preprocessing.scale(X1_test)\n",
    "X2_full=preprocessing.scale(X1_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 3\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.6,random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###FOR SGD THINGS for GloVe embedding\n",
    "##maybe add W_regularizer=l2(0.01)\n",
    "def create_model_w2v(neurons=1000,neurons2=100,  ##20 et 10\n",
    "                     init_mode='he_uniform',activation='sigmoid',\n",
    "                     learn_rate=0.1,momentum=0.5,dropout_rate=0.2,\n",
    "                     weight_constraint=2,weight_regularizer=2):\n",
    "    \n",
    "    ## the input layer must have the input_dim numbers of input\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=50, init='he_uniform', activation=activation,W_regularizer=l2(weight_regularizer),W_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons2, init='he_uniform', activation=activation))\n",
    "    model.add(Dense(1, init='he_uniform', activation=activation))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [0,1,2]\n",
    "dropout_rate = [0,0.3]\n",
    "weight_regularizer=[0,1,2,3]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint,weight_regularizer=weight_regularizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRID SEARCH Learn rate and momentum\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=10, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.3,0.5,1]\n",
    "momentum = [ 0,0.1,0.5]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### How to Tune the Number of Neurons in the Hidden Layer\n",
    "## create model\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [20,1000]\n",
    "neurons2= [5,100]\n",
    "param_grid = dict(neurons=neurons,neurons2=neurons2)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=2, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(7, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(7, 1)\n",
    "\n",
    "neurons=np.asarray(neurons).reshape(7, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [2,3]\n",
    "dropout_rate = [0.2,0.3]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X2, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model_tt):\n",
    "    # saving model\n",
    "    json_model = model_tt.model.to_json()\n",
    "    open('model_architecture.json', 'w').write(json_model)\n",
    "    # saving weights\n",
    "    model_tt.model.save_weights('model_weights.h5', overwrite=True) \n",
    "\n",
    "def load_model():\n",
    "    # loading model\n",
    "    model = model_from_json(open('model_architecture.json').read())\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "mybestmodel = KerasClassifier(build_fn=create_model_glove)\n",
    "mybestmodel.fit(X1_full, y_full,validation_split=0.2, nb_epoch=50, batch_size=20)  #can add verbose=0 for no wait bar\n",
    "save_model(mybestmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Cross Valid on full data:\n",
    "mybestmodel.score(X2_full,y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mybestmodel=load_model()\n",
    "mybestmodel.evaluate(X2_full,y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = mybestmodel.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_rendu=[]\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>= 0.5:\n",
    "        y_rendu.append(1)\n",
    "    else: y_rendu.append(-1)\n",
    "        \n",
    "OUTPUT_PATH = 'prediction.csv' \n",
    "ids_test=[i+1 for i in range(len(y_rendu))]\n",
    "create_csv_submission(ids_test, y_rendu, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
