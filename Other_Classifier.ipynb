{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGING NEGATIVE AND POSITIVE DATASETS TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['twitter-datasets/train_neg.txt', 'twitter-datasets/train_pos.txt'] # Names of the files to open\n",
    "with open('twitter-datasets/train_total.txt', 'w') as outfile: # Names of the output file\n",
    "    \n",
    "    # Loop in the input files\n",
    "    for fname in filenames:  \n",
    "        \n",
    "        # Open the input files and save each line in the output one\n",
    "        with open(fname) as infile: \n",
    "            \n",
    "            # loop on all the lines in a specific file\n",
    "            for line in infile:\n",
    "                \n",
    "                # write this line in another file\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5d4858d639e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitter_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/matteo/Dropbox/PCML_P1/Project2/CODE_FINAL/twitter_processing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from misc_tools import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from twitter_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first Load tweets, vocabulary, co-occurence matrix and word embedding created in the DataCreation.ipynb File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the word embedding representation of each word\n",
    "w_emb = np.load('embeddings_nltk.npy')\n",
    "\n",
    "# load vocab\n",
    "with open('vocab_nltk.pkl', 'rb') as f:\n",
    "    tot_vocab = pickle.load(f)\n",
    "\n",
    "# load the coocurrence matrix     \n",
    "with open('cooc_nltk.pkl', 'rb') as f:\n",
    "    cooc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove data (D = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the train file tweets Glove representation\n",
    "X = np.load('tweets_emb_nltk.npy')\n",
    "y = np.load('tweets_sol_nltk.npy')\n",
    "\n",
    "# load the full file tweets Glove representation\n",
    "X_full = np.load('tweets_emb_nltk_full.npy')\n",
    "y_full = np.load('tweets_sol_nltk_full.npy')\n",
    "\n",
    "# load the test file tweets Glove representation\n",
    "X_test = np.load('tweets_emb_nltk_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the train file tweets word2vec representation\n",
    "X = np.load('X_w2v.npy')\n",
    "y = np.load('y_w2v.npy')\n",
    "\n",
    "# Apply PCA to our features to reduce to 50 instead of 300\n",
    "pca = PCA(n_components = 50 , svd_solver = 'full')\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Array of the different polynomial degrees with which we are fitting our model\n",
    "degree = [1,2,3,4]\n",
    "\n",
    "# for each polynomial degree\n",
    "for d in degree:\n",
    "    \n",
    "    # We build our polynomial data representation \n",
    "    phi = build_poly(X, d)\n",
    "    \n",
    "    # normialize our data\n",
    "    phi = normalizator(phi)\n",
    "    \n",
    "    # Penalty L2 and Stochastic average gradient descent\n",
    "    model_logistic = LogisticRegression(penalty = 'l2' , C = 10, solver='sag') \n",
    "    scores = cross_val_score(model_logistic, phi, y, cv = 5)\n",
    "    print('Avg score: ' , np.mean(scores) , ' +/- ' , np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize our data \n",
    "X = normalizator(X)\n",
    "\n",
    "# different values of our gamma hyper parameter\n",
    "gam_vec = [0.00005, 0.001, 0.1]\n",
    "\n",
    "# for each value of gamma\n",
    "for g in gam_vec:\n",
    "    \n",
    "    # initialize our model\n",
    "    kernel_svm = SVC(gamma = g) # by default radial kernel. Set 'kernel=poly' for polynomial one\n",
    "    \n",
    "    # reduce the train set as Svm takes a lot of time to compile\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.9,random_state=1)\n",
    "    \n",
    "    # fit our models with reduced train data\n",
    "    kernel_svm.fit(X_train , y_train)\n",
    "    \n",
    "    # 5 times cross validation with our kernel_svm\n",
    "    scores = cross_val_score(kernel_svm,X_train,y_train,cv=5)\n",
    "    print('Mean score: ', np.mean(scores), ' +/- ', np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILDING THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess our test set\n",
    "tweets_data_test, count_test = preprocess(\"twitter-datasets/test_data.txt\")\n",
    "\n",
    "# process our test set (vector representation)\n",
    "X_test = build_test_set(tot_vocab , w_emb , tweets_data_test)\n",
    "\n",
    "# normalize the test vectors\n",
    "X_test = normalizator(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVING THE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict \n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "pred_file = 'predict.csv'\n",
    "create_csv_submission(y_test,pred_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For GloVe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.999, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapt Glove with Neural networks\n",
    "\n",
    "def create_model_glove(neurons=15,neurons2=5,\n",
    "                     init_mode='he_uniform',activation='sigmoid',\n",
    "                     learn_rate=0.1,momentum=0.5,dropout_rate=0.3,\n",
    "                     weight_constraint=0, weight_regularizer=0.0001):\n",
    "    \n",
    "   \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the input, hidden and output layers \n",
    "    model.add(Dense(neurons, input_dim = 20, init = 'he_uniform', activation = activation, W_regularizer = l2(weight_regularizer)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons2, init = 'he_uniform', activation = activation))\n",
    "    model.add(Dense(1, init ='he_uniform', activation = activation))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = SGD(lr = learn_rate, momentum = momentum)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.557789 using {'momentum': 0.1, 'learn_rate': 1}\n",
      "0.442211 (0.048098) with: {'momentum': 0, 'learn_rate': 0.3}\n",
      "0.442211 (0.048098) with: {'momentum': 0.1, 'learn_rate': 0.3}\n",
      "0.452261 (0.050341) with: {'momentum': 0.5, 'learn_rate': 0.3}\n",
      "0.437186 (0.041320) with: {'momentum': 0, 'learn_rate': 0.5}\n",
      "0.437186 (0.041320) with: {'momentum': 0.1, 'learn_rate': 0.5}\n",
      "0.507538 (0.074808) with: {'momentum': 0.5, 'learn_rate': 0.5}\n",
      "0.507538 (0.074808) with: {'momentum': 0, 'learn_rate': 1}\n",
      "0.557789 (0.048098) with: {'momentum': 0.1, 'learn_rate': 1}\n",
      "0.492462 (0.074808) with: {'momentum': 0.5, 'learn_rate': 1}\n"
     ]
    }
   ],
   "source": [
    "# grid search Learn rate and momentum\n",
    "model = KerasClassifier(build_fn = create_model_glove, nb_epoch = 10, batch_size = 20, verbose = 0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.3, 0.5, 1]\n",
    "momentum = [0,0.1, 0.5]\n",
    "param_grid = dict(learn_rate = learn_rate, momentum = momentum)\n",
    "\n",
    "# create and fit gridSearch\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the Neuron Activation Function\n",
    "model = KerasClassifier(build_fn = create_model_glove, nb_epoch = 10, batch_size=20, verbose = 0)\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation = activation)\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Tune the Number of Neurons in the Hidden Layer\n",
    "\n",
    "## create model\n",
    "model = KerasClassifier(build_fn = create_model_glove, nb_epoch = 10, batch_size = 20, verbose = 0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [5, 10, 15, 20]\n",
    "neurons2= [5]\n",
    "param_grid = dict(neurons = neurons, neurons2 = neurons2)\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(4, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(4, 1)\n",
    "neurons = np.asarray(neurons).reshape(4, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(5, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(5, 1)\n",
    "\n",
    "neurons=np.asarray(neurons).reshape(5, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_glove, nb_epoch=10, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [0]\n",
    "weight_regularizer= [0.00001,0.0001,0.001,0.01,0.1,1]\n",
    "dropout_rate = [0.3]\n",
    "param_grid = dict(dropout_rate=dropout_rate,weight_regularizer=weight_regularizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(6, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(6, 1)\n",
    "\n",
    "neurons=np.asarray(weight_regularizer).reshape(6, 1)\n",
    "\n",
    "\n",
    "plt.scatter(weight_regularizer, scores)\n",
    "plt.errorbar(weight_regularizer,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('L2 Regularizer parameter')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.xscale('log', nonposy='clip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INITIALISATION OF W\n",
    "\n",
    "#create model\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=10, batch_size=100, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Word2Vec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with data from word2vec\n",
    "X=np.load('X_w2v.npy')\n",
    "y=np.load('y_w2v.npy')\n",
    "\n",
    "X_full = np.load('X_w2v_full.npy')\n",
    "y_full = np.load('y_w2v_full.npy')\n",
    "\n",
    "X_test = np.load('X_test_w2v.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 : Principal component via Singular value decompo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we kept only the 50 first that keep the 98% of variance\n",
    "pca = PCA(n_components=50,svd_solver='full')\n",
    "pca.fit(X)\n",
    "X1 = pca.transform(X)\n",
    "X1_test = pca.transform(X_test)\n",
    "\n",
    "# print the percentage of variance captured with those features\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1_full = pca.transform(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2 : scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step2 : scaling data\n",
    "X2 = preprocessing.scale(X1)\n",
    "X2_test = preprocessing.scale(X1_test)\n",
    "X2_full = preprocessing.scale(X1_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data \n",
    "X_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.6,random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR SGD THINGS for word2vec embedding\n",
    "\n",
    "def create_model_w2v(neurons=1000,neurons2=100,  ##20 et 10\n",
    "                     init_mode='he_uniform',activation='sigmoid',\n",
    "                     learn_rate=0.1,momentum=0.5,dropout_rate=0.2,\n",
    "                     weight_constraint=2,weight_regularizer=2):\n",
    "    \n",
    "    ## the input layer must have the input_dim numbers of input\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=50, init='he_uniform', activation=activation,W_regularizer=l2(weight_regularizer),W_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons2, init='he_uniform', activation=activation))\n",
    "    model.add(Dense(1, init='he_uniform', activation=activation))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "weight_constraint = [0,1,2]\n",
    "dropout_rate = [0,0.3]\n",
    "weight_regularizer=[0,1,2,3]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint,weight_regularizer=weight_regularizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRID SEARCH Learn rate and momentum\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=10, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.3,0.5,1]\n",
    "momentum = [ 0,0.1,0.5]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Tune the Number of Neurons in the Hidden Layer\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [20,1000]\n",
    "neurons2= [5,100]\n",
    "param_grid = dict(neurons=neurons,neurons2=neurons2)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=2, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.asarray(scores).reshape(7, 1)\n",
    "stds = np.asarray(grid_result.cv_results_['std_test_score'])\n",
    "stds = stds.reshape(7, 1)\n",
    "\n",
    "neurons=np.asarray(neurons).reshape(7, 1)\n",
    "print(neurons.shape)\n",
    "plt.scatter(neurons, scores)#,stds)#, label='N2: ' + str(i))\n",
    "plt.errorbar(neurons,scores,yerr=stds, linestyle=\"None\")\n",
    "plt.legend()\n",
    "plt.xlabel('Neurons on layer 1')\n",
    "plt.ylabel('Mean score')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout rate & Weight constraint\n",
    "model = KerasClassifier(build_fn=create_model_w2v, nb_epoch=5, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [2,3]\n",
    "dropout_rate = [0.2,0.3]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X2, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model_tt):\n",
    "    \n",
    "    # saving model\n",
    "    json_model = model_tt.model.to_json()\n",
    "    open('model_architecture.json', 'w').write(json_model)\n",
    "    \n",
    "    # saving weights\n",
    "    model_tt.model.save_weights('model_weights.h5', overwrite=True) \n",
    "\n",
    "def load_model():\n",
    "    \n",
    "    # loading model\n",
    "    model = model_from_json(open('model_architecture.json').read())\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "mybestmodel = KerasClassifier(build_fn=create_model_glove)\n",
    "mybestmodel.fit(X1_full, y_full,validation_split=0.2, nb_epoch=50, batch_size=20)  #can add verbose=0 for no wait bar\n",
    "save_model(mybestmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation on full data:\n",
    "mybestmodel.score(X2_full,y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = mybestmodel.predict(X2_test)\n",
    "y_rendu = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    \n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_rendu.append(1)\n",
    "    else: \n",
    "        y_rendu.append(-1)\n",
    "        \n",
    "OUTPUT_PATH = 'prediction.csv' \n",
    "ids_test = [i+1 for i in range(len(y_rendu))]\n",
    "create_csv_submission(ids_test, y_rendu, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
