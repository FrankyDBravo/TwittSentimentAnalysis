{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creation of X and Y data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from twitter_processing import *\n",
    "from coocme import *\n",
    "from glove_emb import *\n",
    "from dataXY import *\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ---- if total file not created (for full):\n",
    "filenames = ['twitter-datasets/train_neg_full.txt', 'twitter-datasets/train_pos_full.txt']\n",
    "with open('twitter-datasets/train_total_full.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "## ----- for small                \n",
    "filenames = ['twitter-datasets/train_neg.txt', 'twitter-datasets/train_pos.txt']\n",
    "with open('twitter-datasets/train_total.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total tweets (neg, pos)  (for  small file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### each line : sperate word of the tweet\n",
    "###count_test gives: all word with all occurences\n",
    "\n",
    "###--- TRAIN DATA (must be same length neg and pos)\n",
    "tweets_data_tot,count_test_tot=preprocess_nltk(\"twitter-datasets/train_total.txt\")\n",
    "## for full:\n",
    "#tweets_data_tot,count_test_tot=preprocess_nltk(\"twitter-datasets/train_total_full.txt\")\n",
    "\n",
    "###---TEST DATA\n",
    "tweets_data_test,count_test_tot=preprocess_nltk(\"twitter-datasets/test_data.txt\")\n",
    "\n",
    "\n",
    "##occurences gives all words and the occurences \n",
    "## but only that occure more than 5 time\n",
    "## weighted gives: the weight of each word in total word\n",
    "tot_occurences, tot_weighted=occurence(count_test_tot,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## vocab gives a list of all the word\n",
    "### tot_vocab is  a dic with word and value associated\n",
    "tot_vocab=create_vocab(tot_occurences,len(tot_occurences))\n",
    "\n",
    "## to get only the words: tot_vocab.keys()\n",
    "\n",
    "# Save a dictionary into a pickle file.\n",
    "with open('vocab_nltk.pkl', 'wb') as f:\n",
    "        pickle.dump(tot_vocab, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create co-occurence matrix (with number of same word in tweet-1 in diago)\n",
    "cooc,cooc_m=create_cooc(tweets_data_tot,tot_vocab)\n",
    "## save as pickle file\n",
    "with open('cooc_nltk.pkl', 'wb') as f:\n",
    "    pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## to see data fram of cooc\n",
    "voc_nice=[tot_occurences[i][0] for i in range(len(tot_occurences)) ]\n",
    "cooc_m=cooc.toarray()\n",
    "## print a data frame\n",
    "cooc_df = pd.DataFrame(cooc_m, index = voc_nice)\n",
    "cooc_df.columns = voc_nice\n",
    "cooc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the word embeddings: [N,D]\n",
    "# D is the embedding dim : default 20\n",
    "w_emb=glove_emb(cooc,15,20)\n",
    "np.save('embeddings_nltk.npy',w_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##--- From here COOC and vocabulary must be created\n",
    "##Create the train data  X & Y for DNN & CNN  in \"tweets_nltk_dnn / cnn\" & \"tweets_nltk_tot_sol\"\n",
    "so\n",
    "create_alloutput(tweets_data_tot,'tweets_nltk_tot')\n",
    "##Create the test data X_test for DNN & CNN in \"tweets_nltk_test_dnn / cnn\"\n",
    "create_test(tweets_data_test,'tweets_nltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomTom/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/TomTom/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tweets_data_tot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df724ff82bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load our tweets and labels (-1 for neg and +1 for pos) associated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_data_tot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_sol_nltk.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_data_tot' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "# load our tweets and labels (-1 for neg and +1 for pos) associated \n",
    "X = tweets_data_tot\n",
    "y = load('tweets_sol_nltk.npy')\n",
    "\n",
    "\n",
    "# number of features of the vector of our word representation\n",
    "dim = 300\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "word2vec = Word2Vec(size = dim, min_count = 10)\n",
    "\n",
    "# Build vocab\n",
    "word2vec.build_vocab(X)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "word2vec.train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vector representation of each tweet by \n",
    "def TweetVector(text, size):\n",
    "    \n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    \n",
    "    # loop on each word in the tweet\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += word2vec[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec = vec / count\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# build our train vectors for each tweet\n",
    "train_vecs = np.concatenate([TweetVector(tweet, dim) for tweet in X])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's now preprocess our test tweets and build our test vectors to predict (for kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "test_tweets, count_test = preprocess(\"twitter-datasets/test_data.txt\")\n",
    "\n",
    "# create dataframe with test  tweets\n",
    "test_df = pd.DataFrame(data = {'Tweets': test_tweets})\n",
    "\n",
    "# split each tweets into arrays of words\n",
    "test_df['Tweets'] = test_df['Tweets'].apply(lambda x : tokenizer.tokenize(x))\n",
    "\n",
    "x_test = test_df['Tweets'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train word2vec on test tweets\n",
    "x_test = load('tweets_emb_nltk_test.npy')\n",
    "word2vec.train(x_test)\n",
    "\n",
    "# Build test tweet vectors then scale\n",
    "test_vecs = np.concatenate([TweetVector(tweet, dim) for tweet in x_test])\n",
    "test_vecs = scale(test_vecs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
